import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False

class FrequencyPreprocessingAnalyzer:
    def __init__(self, df, col1, col2):
        """
        ë¹ˆë„ìˆ˜ ë°ì´í„° ì „ì²˜ë¦¬ ë°©ë²•ë³„ ìƒê´€ê´€ê³„ ë¶„ì„ í´ë˜ìŠ¤
        
        Parameters:
        df: pandas DataFrame
        col1: ì²« ë²ˆì§¸ ë¹ˆë„ìˆ˜ ì»¬ëŸ¼ëª…
        col2: ë‘ ë²ˆì§¸ ë¹ˆë„ìˆ˜ ì»¬ëŸ¼ëª…
        """
        self.df = df.copy()
        self.col1 = col1
        self.col2 = col2
        
        # ê²°ì¸¡ê°’ ì œê±°
        self.df_clean = self.df[[col1, col2]].dropna()
        
    def apply_preprocessing_methods(self):
        """ë‹¤ì–‘í•œ ì „ì²˜ë¦¬ ë°©ë²• ì ìš©"""
        methods = {}
        
        # 1. ì›ë³¸ ë°ì´í„°
        methods['original'] = {
            'col1': self.df_clean[self.col1],
            'col2': self.df_clean[self.col2]
        }
        
        # 2. ë¡œê·¸ ë³€í™˜ (log1p)
        methods['log_transform'] = {
            'col1': np.log1p(self.df_clean[self.col1]),
            'col2': np.log1p(self.df_clean[self.col2])
        }
        
        # 3. ì œê³±ê·¼ ë³€í™˜
        methods['sqrt_transform'] = {
            'col1': np.sqrt(self.df_clean[self.col1]),
            'col2': np.sqrt(self.df_clean[self.col2])
        }
        
        # 4. í‘œì¤€í™” (StandardScaler)
        scaler_std = StandardScaler()
        std_data = scaler_std.fit_transform(self.df_clean[[self.col1, self.col2]])
        methods['standardization'] = {
            'col1': std_data[:, 0],
            'col2': std_data[:, 1]
        }
        
        # 5. ì •ê·œí™” (MinMaxScaler)
        scaler_minmax = MinMaxScaler()
        minmax_data = scaler_minmax.fit_transform(self.df_clean[[self.col1, self.col2]])
        methods['normalization'] = {
            'col1': minmax_data[:, 0],
            'col2': minmax_data[:, 1]
        }
        
        # 6. ë¡œë²„ìŠ¤íŠ¸ ìŠ¤ì¼€ì¼ë§
        scaler_robust = RobustScaler()
        robust_data = scaler_robust.fit_transform(self.df_clean[[self.col1, self.col2]])
        methods['robust_scaling'] = {
            'col1': robust_data[:, 0],
            'col2': robust_data[:, 1]
        }
        
        # 7. ìˆœìœ„ ë³€í™˜
        methods['rank_transform'] = {
            'col1': self.df_clean[self.col1].rank(),
            'col2': self.df_clean[self.col2].rank()
        }
        
        return methods
    
    def calculate_correlations(self, methods):
        """ê° ì „ì²˜ë¦¬ ë°©ë²•ë³„ ìƒê´€ê³„ìˆ˜ ê³„ì‚°"""
        results = {}
        
        for method_name, data in methods.items():
            col1_data = data['col1']
            col2_data = data['col2']
            
            # í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜
            pearson_corr, pearson_p = stats.pearsonr(col1_data, col2_data)
            
            # ìŠ¤í”¼ì–´ë§Œ ìƒê´€ê³„ìˆ˜
            spearman_corr, spearman_p = stats.spearmanr(col1_data, col2_data)
            
            # ì¼„ë‹¬ íƒ€ìš°
            kendall_corr, kendall_p = stats.kendalltau(col1_data, col2_data)
            
            results[method_name] = {
                'pearson': {'correlation': pearson_corr, 'p_value': pearson_p},
                'spearman': {'correlation': spearman_corr, 'p_value': spearman_p},
                'kendall': {'correlation': kendall_corr, 'p_value': kendall_p}
            }
        
        return results
    
    def plot_comparison(self, methods, results):
        """ì „ì²˜ë¦¬ ë°©ë²•ë³„ ë¹„êµ ì‹œê°í™”"""
        fig, axes = plt.subplots(2, 4, figsize=(20, 10))
        axes = axes.flatten()
        
        method_names = list(methods.keys())
        
        for i, method_name in enumerate(method_names):
            if i >= 8:  # ìµœëŒ€ 8ê°œ ê·¸ë˜í”„
                break
                
            data = methods[method_name]
            ax = axes[i]
            
            # ì‚°ì ë„
            ax.scatter(data['col1'], data['col2'], alpha=0.6, s=20)
            
            # ìƒê´€ê³„ìˆ˜ í‘œì‹œ
            pearson_corr = results[method_name]['pearson']['correlation']
            ax.set_title(f'{method_name}\nPearson r = {pearson_corr:.3f}')
            ax.grid(True, alpha=0.3)
            
            # ì¶• ë ˆì´ë¸” ê°„ì†Œí™”
            if i % 4 == 0:  # ì™¼ìª½ ì—´
                ax.set_ylabel('ë¹ˆë„ìˆ˜')
            if i >= 4:  # ì•„ë˜ìª½ í–‰
                ax.set_xlabel('ë¹ˆë„ìˆ˜')
        
        plt.tight_layout()
        plt.show()
    
    def plot_correlation_comparison(self, results):
        """ìƒê´€ê³„ìˆ˜ ë¹„êµ ì°¨íŠ¸"""
        methods = list(results.keys())
        
        # ìƒê´€ê³„ìˆ˜ ì¶”ì¶œ
        pearson_corrs = [results[m]['pearson']['correlation'] for m in methods]
        spearman_corrs = [results[m]['spearman']['correlation'] for m in methods]
        kendall_corrs = [results[m]['kendall']['correlation'] for m in methods]
        
        # ì‹œê°í™”
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # ìƒê´€ê³„ìˆ˜ ë¹„êµ
        x = np.arange(len(methods))
        width = 0.25
        
        ax1.bar(x - width, pearson_corrs, width, label='Pearson', alpha=0.8)
        ax1.bar(x, spearman_corrs, width, label='Spearman', alpha=0.8)
        ax1.bar(x + width, kendall_corrs, width, label='Kendall', alpha=0.8)
        
        ax1.set_xlabel('ì „ì²˜ë¦¬ ë°©ë²•')
        ax1.set_ylabel('ìƒê´€ê³„ìˆ˜')
        ax1.set_title('ì „ì²˜ë¦¬ ë°©ë²•ë³„ ìƒê´€ê³„ìˆ˜ ë¹„êµ')
        ax1.set_xticks(x)
        ax1.set_xticklabels(methods, rotation=45, ha='right')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # p-value ë¹„êµ (í”¼ì–´ìŠ¨ ê¸°ì¤€)
        p_values = [results[m]['pearson']['p_value'] for m in methods]
        ax2.bar(methods, p_values, alpha=0.8, color='orange')
        ax2.set_xlabel('ì „ì²˜ë¦¬ ë°©ë²•')
        ax2.set_ylabel('p-value')
        ax2.set_title('í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ p-value ë¹„êµ')
        ax2.set_xticklabels(methods, rotation=45, ha='right')
        ax2.axhline(y=0.05, color='red', linestyle='--', label='p=0.05')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def print_detailed_analysis(self, results):
        """ìƒì„¸ ë¶„ì„ ê²°ê³¼ ì¶œë ¥"""
        print("=" * 80)
        print("ë¹ˆë„ìˆ˜ ë°ì´í„° ì „ì²˜ë¦¬ ë°©ë²•ë³„ ìƒê´€ê´€ê³„ ë¶„ì„ ê²°ê³¼")
        print("=" * 80)
        
        # ê¸°ë³¸ í†µê³„
        print(f"\nğŸ“Š ì›ë³¸ ë°ì´í„° ê¸°ë³¸ í†µê³„:")
        print(f"ë°ì´í„° ê°œìˆ˜: {len(self.df_clean)}")
        print(f"{self.col1} - í‰ê· : {self.df_clean[self.col1].mean():.2f}, í‘œì¤€í¸ì°¨: {self.df_clean[self.col1].std():.2f}")
        print(f"{self.col2} - í‰ê· : {self.df_clean[self.col2].mean():.2f}, í‘œì¤€í¸ì°¨: {self.df_clean[self.col2].std():.2f}")
        
        # ê° ë°©ë²•ë³„ ê²°ê³¼
        print(f"\nğŸ” ì „ì²˜ë¦¬ ë°©ë²•ë³„ ìƒê´€ê³„ìˆ˜:")
        print(f"{'ë°©ë²•':<20} {'Pearson':<10} {'Spearman':<10} {'Kendall':<10}")
        print("-" * 60)
        
        for method_name, result in results.items():
            pearson = result['pearson']['correlation']
            spearman = result['spearman']['correlation']
            kendall = result['kendall']['correlation']
            
            print(f"{method_name:<20} {pearson:<10.4f} {spearman:<10.4f} {kendall:<10.4f}")
        
        # ê¶Œì¥ì‚¬í•­
        print(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
        print("1. ë¡œê·¸ ë³€í™˜: ë¹ˆë„ìˆ˜ ë°ì´í„°ì˜ ê¸°ë³¸ì ì¸ ì „ì²˜ë¦¬ ë°©ë²•")
        print("2. ìˆœìœ„ ë³€í™˜: ì´ìƒì¹˜ì— ê°•í•˜ê³  ë¹„ì„ í˜• ê´€ê³„ë„ í¬ì°©")
        print("3. ë¡œë²„ìŠ¤íŠ¸ ìŠ¤ì¼€ì¼ë§: ì´ìƒì¹˜ê°€ ë§ì€ ê²½ìš° ìœ ìš©")
        print("4. ì›ë³¸ ë°ì´í„°: ì„ í˜• ê´€ê³„ê°€ ëª…í™•í•œ ê²½ìš°ì—ë§Œ ì‚¬ìš©")
        
        print("=" * 80)
    
    def run_full_analysis(self):
        """ì „ì²´ ë¶„ì„ ì‹¤í–‰"""
        print("ë¹ˆë„ìˆ˜ ë°ì´í„° ì „ì²˜ë¦¬ ë°©ë²•ë³„ ìƒê´€ê´€ê³„ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        # ì „ì²˜ë¦¬ ë°©ë²• ì ìš©
        methods = self.apply_preprocessing_methods()
        
        # ìƒê´€ê³„ìˆ˜ ê³„ì‚°
        results = self.calculate_correlations(methods)
        
        # ê²°ê³¼ ì¶œë ¥
        self.print_detailed_analysis(results)
        
        # ì‹œê°í™”
        self.plot_comparison(methods, results)
        self.plot_correlation_comparison(results)
        
        return methods, results

# ì‚¬ìš© ì˜ˆì‹œ í•¨ìˆ˜
def analyze_frequency_preprocessing(df, col1, col2):
    """
    ë¹ˆë„ìˆ˜ ë°ì´í„° ì „ì²˜ë¦¬ ë°©ë²•ë³„ ìƒê´€ê´€ê³„ ë¶„ì„
    
    Parameters:
    df: pandas DataFrame
    col1: ì²« ë²ˆì§¸ ë¹ˆë„ìˆ˜ ì»¬ëŸ¼ëª…
    col2: ë‘ ë²ˆì§¸ ë¹ˆë„ìˆ˜ ì»¬ëŸ¼ëª…
    """
    analyzer = FrequencyPreprocessingAnalyzer(df, col1, col2)
    return analyzer.run_full_analysis()

# ì˜ˆì‹œ ì‚¬ìš©ë²•
if __name__ == "__main__":
    # ì˜ˆì‹œ ë°ì´í„° ìƒì„± (ì‹¤ì œ ë¹ˆë„ìˆ˜ ë¶„í¬ì™€ ìœ ì‚¬í•˜ê²Œ)
    np.random.seed(42)
    n_samples = 1000
    
    # ê³ ë¹ˆë„ ë‹¨ì–´ë“¤ (ì†Œìˆ˜)
    high_freq_words = np.random.poisson(lam=100, size=50)
    
    # ì¤‘ë¹ˆë„ ë‹¨ì–´ë“¤
    mid_freq_words = np.random.poisson(lam=20, size=200)
    
    # ì €ë¹ˆë„ ë‹¨ì–´ë“¤ (ëŒ€ë¶€ë¶„)
    low_freq_words = np.random.poisson(lam=3, size=750)
    
    # ì½”í¼ìŠ¤1: ë¹ˆë„ìˆ˜ ë¶„í¬
    freq1 = np.concatenate([high_freq_words, mid_freq_words, low_freq_words])
    
    # ì½”í¼ìŠ¤2: ì½”í¼ìŠ¤1ê³¼ ìƒê´€ê´€ê³„ê°€ ìˆì§€ë§Œ ìŠ¤ì¼€ì¼ì´ ë‹¤ë¥¸ ë¹ˆë„ìˆ˜
    freq2 = freq1 * 0.6 + np.random.normal(0, 5, len(freq1))
    freq2 = np.maximum(freq2, 0)  # ìŒìˆ˜ ë°©ì§€
    
    example_df = pd.DataFrame({
        'corpus1_freq': freq1,
        'corpus2_freq': freq2
    })
    
    print("ì˜ˆì‹œ ë¹ˆë„ìˆ˜ ë°ì´í„°ë¡œ ì „ì²˜ë¦¬ ë°©ë²•ë³„ ìƒê´€ê´€ê³„ ë¶„ì„ ì‹¤í–‰:")
    methods, results = analyze_frequency_preprocessing(example_df, 'corpus1_freq', 'corpus2_freq') 